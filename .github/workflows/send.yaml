name: Daily Crawler with Email Notification

on:
  schedule:
    # 每天UTC时间00:00运行（北京时间08:00）
    - cron: '0 0 * * *'
  workflow_dispatch:  # 允许手动触发
    inputs:
      manual_run:
        description: '手动运行爬虫'
        required: false
        default: 'true'

jobs:
  run-crawler-and-notify:
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
      
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.10'
        
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install requests lxml
        # 如果有 requirements.txt，使用下面的命令
        # pip install -r requirements.txt
        
    - name: Run crawler script
      id: crawler
      run: |
        # 运行你的爬虫脚本，假设文件名为 crawler.py
        OUTPUT=$(python crawler.py)
        echo "爬虫输出: $OUTPUT"
        # 将输出保存到环境变量，以便后续步骤使用
        echo "CRAWLER_OUTPUT=$OUTPUT" >> $GITHUB_ENV
        
    - name: Send email notification
      uses: dawidd6/action-send-mail@v3
      with:
        # 邮箱服务器配置（以QQ邮箱为例）
        server_address: smtp.qq.com
        server_port: 465
        # 发件人邮箱（需要配置到GitHub Secrets）
        username: ${{ secrets.EMAIL_USERNAME }}
        password: ${{ secrets.EMAIL_PASSWORD }}
        
        # 邮件内容
        subject: "爬虫结果通知 - ${{ github.workflow }}"
        body: |
          爬虫执行时间: ${{ steps.crawler.outcome }} UTC
          
          爬虫输出结果:
          ${{ env.CRAWLER_OUTPUT }}
          
          工作流详情: https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }}
          
        # 收件人邮箱
        to: ${{ secrets.NOTIFICATION_EMAIL }}
        from: GitHub Actions Crawler
        # 使用SSL加密
        secure: true
